
@Manual{corHMM,
    title = {corHMM: Hidden Markov Models of Character Evolution},
    author = {Jeremy Beaulieu and Brian O'Meara and Jeffrey Oliver and James Boyko},
    year = {2021},
    note = {R package version 2.6.1 (BMB fork)},
    url = {https://github.com/bbolker/corHMM/}
  }

@article{pagel_bayesian_2006,
	title = {Bayesian {Analysis} of {Correlated} {Evolution} of {Discrete} {Characters} by {Reversible}‐{Jump} {Markov} {Chain} {Monte} {Carlo}},
	volume = {167},
	issn = {0003-0147, 1537-5323},
	url = {http://www.journals.uchicago.edu/doi/10.1086/503444},
	doi = {10.1086/503444},
	abstract = {We describe a Bayesian method for investigating correlated evolution of discrete binary traits on phylogenetic trees. The method ﬁts a continuous-time Markov model to a pair of traits, seeking the best ﬁtting models that describe their joint evolution on a phylogeny. We employ the methodology of reversible-jump (RJ) Markov chain Monte Carlo to search among the large number of possible models, some of which conform to independent evolution of the two traits, others to correlated evolution. The RJ Markov chain visits these models in proportion to their posterior probabilities, thereby directly estimating the support for the hypothesis of correlated evolution. In addition, the RJ Markov chain simultaneously estimates the posterior distributions of the rate parameters of the model of trait evolution. These posterior distributions can be used to test among alternative evolutionary scenarios to explain the observed data. All results are integrated over a sample of phylogenetic trees to account for phylogenetic uncertainty. We implement the method in a program called RJ Discrete and illustrate it by analyzing the question of whether mating system and advertisement of estrus by females have coevolved in the Old World monkeys and great apes.},
	language = {en},
	number = {6},
	urldate = {2021-05-06},
	journal = {The American Naturalist},
	author = {Pagel, Mark and Meade, Andrew},
	month = jun,
	year = {2006},
	pages = {808--825},
	file = {Pagel and Meade - 2006 - Bayesian Analysis of Correlated Evolution of Discr.pdf:/home/bolker/Documents/zotero_new/storage/KXN5PKYP/Pagel and Meade - 2006 - Bayesian Analysis of Correlated Evolution of Discr.pdf:application/pdf}
}

@incollection{beaulieu_hidden_2014,
	address = {Berlin, Heidelberg},
	title = {Hidden {Markov} {Models} for {Studying} the {Evolution} of {Binary} {Morphological} {Characters}},
	isbn = {978-3-662-43550-2},
	url = {https://doi.org/10.1007/978-3-662-43550-2_16},
	abstract = {Biologists now have the capability of building large phylogenetic trees consisting of tens of thousands of species, from which important comparative questions can be addressed. However, to the extent that biologists have applied these large trees to comparative data, it is clear that current methods, such as those that deal with the evolution of binary morphological characters, make unrealistic assumptions about how these characters are modeled. As phylogenies increase both in size and scope, it is likely that the lability of a binary character will differ significantly among lineages. In this chapter, we describe how a new generalized model, which we refer to as the “hidden rates model” (HRM), can be used to identify different rates of evolution in a discrete binary character along different branches of a phylogeny. The HRM is part of a class of models that are more broadly known as Hidden Markov models because it presupposes that unobserved “hidden” rate classes underlie each observed state and that each rate class represents potentially different transition rates to and from these observed states. As we discuss, the recognition and accommodation of this heterogeneity can provide a robust picture of binary character evolution.},
	language = {en},
	urldate = {2021-05-06},
	booktitle = {Modern {Phylogenetic} {Comparative} {Methods} and {Their} {Application} in {Evolutionary} {Biology}: {Concepts} and {Practice}},
	publisher = {Springer},
	author = {Beaulieu, Jeremy M. and O’Meara, Brian C.},
	editor = {Garamszegi, László Zsolt},
	year = {2014},
	doi = {10.1007/978-3-662-43550-2_16},
	keywords = {Binary Character, Herbaceous Condition, Hidden Rate, Observed Character States, Rate Classes},
	pages = {395--408},
	file = {Springer Full Text PDF:/home/bolker/Documents/zotero_new/storage/RUX9QUHM/Beaulieu and O’Meara - 2014 - Hidden Markov Models for Studying the Evolution of.pdf:application/pdf}
}

@article{beaulieu_identifying_2013,
	title = {Identifying {Hidden} {Rate} {Changes} in the {Evolution} of a {Binary} {Morphological} {Character}: {The} {Evolution} of {Plant} {Habit} in {Campanulid} {Angiosperms}},
	volume = {62},
	issn = {1063-5157},
	shorttitle = {Identifying {Hidden} {Rate} {Changes} in the {Evolution} of a {Binary} {Morphological} {Character}},
	url = {https://doi.org/10.1093/sysbio/syt034},
	doi = {10.1093/sysbio/syt034},
	abstract = {The growth of phylogenetic trees in scope and in size is promising from the standpoint of understanding a wide variety of evolutionary patterns and processes. With trees comprised of larger, older, and globally distributed clades, it is likely that the lability of a binary character will differ significantly among lineages, which could lead to errors in estimating transition rates and the associated inference of ancestral states. Here we develop and implement a new method for identifying different rates of evolution in a binary character along different branches of a phylogeny. We illustrate this approach by exploring the evolution of growth habit in Campanulidae, a flowering plant clade containing some 35,000 species. The distribution of woody versus herbaceous species calls into question the use of traditional models of binary character evolution. The recognition and accommodation of changes in the rate of growth form evolution in different lineages demonstrates, for the first time, a robust picture of growth form evolution across a very large, very old, and very widespread flowering plant clade. [Binary character; Campanulidae; comparative methods; flowering plants; growth habit; herbaceous; Hidden rates model; woody.]},
	number = {5},
	urldate = {2021-05-06},
	journal = {Systematic Biology},
	author = {Beaulieu, Jeremy M. and O'Meara, Brian C. and Donoghue, Michael J.},
	month = sep,
	year = {2013},
	pages = {725--737},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/RAH6D7ZG/Beaulieu et al. - 2013 - Identifying Hidden Rate Changes in the Evolution o.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/5S3TRBJH/1685276.html:text/html}
}



@article{meade_bayestraits_2016,
	title = {{BayesTraits} {V3} user manual},
	url = {https://usermanual.wiki/Pdf/BayesTraitsV3Manual.1382538873.pdf},
	language = {en},
	urldate = {2021-07-08},
	author = {Meade, Andrew and Pagel, Mark},
	month = nov,
	year = {2016},
	pages = {81}
}


@article{thorson_uniform_2017,
	title = {Uniform, uninformed or misinformed?: {The} lingering challenge of minimally informative priors in data-limited {Bayesian} stock assessments},
	volume = {194},
	issn = {01657836},
	shorttitle = {Uniform, uninformed or misinformed?},
	url = {https://journals.scholarsportal.info/details/01657836/v194icomplete/164_uuomtlpidbsa.xml},
	doi = {10.1016/j.fishres.2017.06.007},
	abstract = {Highlights • Bayesian methods are widely used for data-poor stock assessment models. • Four case-studies and a simulation experiment show that Bayesian data-poor assessments can be highly sensitive to unjustified decisions about priors. • Specifically, the arbitrary upper bound on a uniform prior on log-maximum recruitment determines results in 3 of 4 case studies. • Potential solutions involve placing informative priors on other parameters or quantities. • Reviewers should avoid prescribing Bayesian methods for data-poor models, for cases when these four solutions are inappropriate. Abstract A Bayesian approach to parameter estimation in fisheries stock assessment is often preferred over maximum likelihood estimates, and fisheries management guidelines also sometimes specify that one or the other paradigm be used. However, important issues remain unresolved for the Bayesian approach to stock assessment despite over 25 years of research, development, and application. Here, we explore the consequence of a common practice in Bayesian assessment models: assigning a uniform prior to the logarithm of the parameter representing population scale (log-carrying capacity for biomass-dynamics models, or log-unfished recruits for age-structured models). First, we explain why the value chosen for the upper bound of this prior will affect parameter estimates and fisheries management advice given two properties that are met for many data-poor stock assessment models. Next, we use three case studies and a simulation experiment to show a substantial impact of this decision for data-limited assessments off the US West Coast. We end by discussing four methods for generating an informative prior on the population scale parameter, but conclude that these will not be suitable for many assessments. In these cases, we advocate that maximum likelihood estimation is a simple way to avoid the use of Bayesian priors that are excessively informative.},
	number = {Complete},
	urldate = {2021-05-13},
	journal = {Fisheries Research},
	author = {Thorson, James T. and Cope, Jason M.},
	year = {2017},
	note = {Publisher: Elsevier},
	keywords = {Maximum likelihood, Bayesian, Stock assessment, Data-limited model, Age-structured surplus production model, Stock synthesis},
	pages = {164--172}
}


@misc{carpenter_computational_2017,
	title = {Computational and statistical issues with uniform interval priors},
	url = {http://andrewgelman.com/2017/11/28/computational-statistical-issues-uniform-interval-priors/},
	abstract = {There are two anti-patterns* for prior specification in Stan programs that can be sourced directly to idioms developed for BUGS. One is the diffuse gamma priors that Andrew’s already written about at length. The second is interval-based priors. Which brings us to today’s post. Interval priors An interval prior is something like this in Stan …},
	language = {en-US},
	urldate = {2018-05-15},
	journal = {Statistical Modeling, Causal Inference, and Social Science},
	author = {Carpenter, Bob},
	month = nov,
	year = {2017}
}


@article{yang_bayesian_2006,
	title = {Bayesian {Estimation} of {Species} {Divergence} {Times} {Under} a {Molecular} {Clock} {Using} {Multiple} {Fossil} {Calibrations} with {Soft} {Bounds}},
	volume = {23},
	issn = {0737-4038},
	url = {https://doi.org/10.1093/molbev/msj024},
	doi = {10.1093/molbev/msj024},
	abstract = {We implement a Bayesian Markov chain Monte Carlo algorithm for estimating species divergence times that uses heterogeneous data from multiple gene loci and accommodates multiple fossil calibration nodes. A birth-death process with species sampling is used to specify a prior for divergence times, which allows easy assessment of the effects of that prior on posterior time estimates. We propose a new approach for specifying calibration points on the phylogeny, which allows the use of arbitrary and flexible statistical distributions to describe uncertainties in fossil dates. In particular, we use soft bounds, so that the probability that the true divergence time is outside the bounds is small but nonzero. A strict molecular clock is assumed in the current implementation, although this assumption may be relaxed. We apply our new algorithm to two data sets concerning divergences of several primate species, to examine the effects of the substitution model and of the prior for divergence times on Bayesian time estimation. We also conduct computer simulation to examine the differences between soft and hard bounds. We demonstrate that divergence time estimation is intrinsically hampered by uncertainties in fossil calibrations, and the error in Bayesian time estimates will not go to zero with increased amounts of sequence data. Our analyses of both real and simulated data demonstrate potentially large differences between divergence time estimates obtained using soft versus hard bounds and a general superiority of soft bounds. Our main findings are as follows. (1) When the fossils are consistent with each other and with the molecular data, and the posterior time estimates are well within the prior bounds, soft and hard bounds produce similar results. (2) When the fossils are in conflict with each other or with the molecules, soft and hard bounds behave very differently; soft bounds allow sequence data to correct poor calibrations, while poor hard bounds are impossible to overcome by any amount of data. (3) Soft bounds eliminate the need for “safe” but unrealistically high upper bounds, which may bias posterior time estimates. (4) Soft bounds allow more reliable assessment of estimation errors, while hard bounds generate misleadingly high precisions when fossils and molecules are in conflict.},
	number = {1},
	urldate = {2021-07-09},
	journal = {Molecular Biology and Evolution},
	author = {Yang, Ziheng and Rannala, Bruce},
	month = jan,
	year = {2006},
	pages = {212--226}
}




@article{rabosky_inverse_2018,
	title = {An inverse latitudinal gradient in speciation rate for marine fishes},
	volume = {559},
	copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-018-0273-1},
	doi = {10.1038/s41586-018-0273-1},
	abstract = {Far more species of organisms are found in the tropics than in temperate and polar regions, but the evolutionary and ecological causes of this pattern remain controversial1,2. Tropical marine fish communities are much more diverse than cold-water fish communities found at higher latitudes3,4, and several explanations for this latitudinal diversity gradient propose that warm reef environments serve as evolutionary ‘hotspots’ for species formation5–8. Here we test the relationship between latitude, species richness and speciation rate across marine fishes. We assembled a time-calibrated phylogeny of all ray-finned fishes (31,526 tips, of which 11,638 had genetic data) and used this framework to describe the spatial dynamics of speciation in the marine realm. We show that the fastest rates of speciation occur in species-poor regions outside the tropics, and that high-latitude fish lineages form new species at much faster rates than their tropical counterparts. High rates of speciation occur in geographical regions that are characterized by low surface temperatures and high endemism. Our results reject a broad class of mechanisms under which the tropics serve as an evolutionary cradle for marine fish diversity and raise new questions about why the coldest oceans on Earth are present-day hotspots of species formation.},
	language = {en},
	number = {7714},
	urldate = {2022-07-27},
	journal = {Nature},
	author = {Rabosky, Daniel L. and Chang, Jonathan and Title, Pascal O. and Cowman, Peter F. and Sallan, Lauren and Friedman, Matt and Kaschner, Kristin and Garilao, Cristina and Near, Thomas J. and Coll, Marta and Alfaro, Michael E.},
	month = jul,
	year = {2018},
	note = {Number: 7714
Publisher: Nature Publishing Group},
	keywords = {Biodiversity, Ecology, Phylogenetics, Speciation},
	pages = {392--395}
}


@article{shi_reconnecting_2021,
	title = {Reconnecting p-{Value} and {Posterior} {Probability} {Under} {One}- and {Two}-{Sided} {Tests}},
	volume = {75},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2020.1717621},
	doi = {10.1080/00031305.2020.1717621},
	abstract = {As a convention, p-value is often computed in frequentist hypothesis testing and compared with the nominal significance level of 0.05 to determine whether or not to reject the null hypothesis. The smaller the p-value, the more significant the statistical test. Under noninformative prior distributions, we establish the equivalence relationship between the p-value and Bayesian posterior probability of the null hypothesis for one-sided tests and, more importantly, the equivalence between the p-value and a transformation of posterior probabilities of the hypotheses for two-sided tests. For two-sided hypothesis tests with a point null, we recast the problem as a combination of two one-sided hypotheses along the opposite directions and establish the notion of a “two-sided posterior probability,” which reconnects with the (two-sided) p-value. In contrast to the common belief, such an equivalence relationship renders p-value an explicit interpretation of how strong the data support the null. Extensive simulation studies are conducted to demonstrate the equivalence relationship between the p-value and Bayesian posterior probability. Contrary to broad criticisms on the use of p-value in evidence-based studies, we justify its utility and reclaim its importance from the Bayesian perspective.},
	number = {3},
	urldate = {2022-07-27},
	journal = {The American Statistician},
	author = {Shi, Haolun and Yin, Guosheng},
	month = jul,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2020.1717621},
	keywords = {Clinical trial, Hypothesis testing, One-sided test, p-Value, Posterior probability, Two-sided test},
	pages = {265--275}
}


@article{makowski_indices_2019,
	title = {Indices of {Effect} {Existence} and {Significance} in the {Bayesian} {Framework}},
	volume = {10},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767},
	abstract = {Turmoil has engulfed psychological science. Causes and consequences of the reproducibility crisis are in dispute. With the hope of addressing some of its aspects, Bayesian methods are gaining increasing attention in psychological science. Some of their advantages, as opposed to the frequentist framework, are the ability to describe parameters in probabilistic terms and explicitly incorporate prior knowledge about them into the model. These issues are crucial in particular regarding the current debate about statistical significance. Bayesian methods are not necessarily the only remedy against incorrect interpretations or wrong conclusions, but there is an increasing agreement that they are one of the keys to avoid such fallacies. Nevertheless, its flexible nature is its power and weakness, for there is no agreement about what indices of “significance” should be computed or reported. This lack of a consensual index or guidelines, such as the frequentist p-value, further contributes to the unnecessary opacity that many non-familiar readers perceive in Bayesian statistics. Thus, this study describes and compares several Bayesian indices, provide intuitive visual representation of their “behavior” in relationship with common sources of variance such as sample size, magnitude of effects and also frequentist significance. The results contribute to the development of an intuitive understanding of the values that researchers report, allowing to draw sensible recommendations for Bayesian statistics description, critical for the standardization of scientific reporting.},
	urldate = {2022-07-21},
	journal = {Frontiers in Psychology},
	author = {Makowski, Dominique and Ben-Shachar, Mattan S. and Chen, S. H. Annabel and Lüdecke, Daniel},
	year = {2019}
}

@book{fox_r_2018,
	address = {Los Angeles},
	edition = {3rd edition},
	title = {An {R} {Companion} to {Applied} {Regression}},
	isbn = {978-1-5443-3647-3},
	language = {English},
	publisher = {SAGE Publications, Inc},
	author = {Fox, John and Weisberg, Sanford},
	month = oct,
	year = {2018},
}

@article{bouckaertDensiTree2010,
  title = {{{DensiTree}}: Making Sense of Sets of Phylogenetic Trees},
  shorttitle = {{{DensiTree}}},
  author = {Bouckaert, Remco R.},
  year = {2010},
  month = may,
  journal = {Bioinformatics},
  volume = {26},
  number = {10},
  pages = {1372--1373},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btq110},
  abstract = {Motivation: Bayesian analysis through programs like BEAST (Drummond and Rumbaut, 2007) and MrBayes (Huelsenbeck et al., 2001) provides a powerful method for reconstruction of evolutionary relationships. One of the benefits of Bayesian methods is that well-founded estimates of uncertainty in models can be made available. So, for example, not only the mean time of a most recent common ancestor (tMRCA) is estimated, but also the spread. This distribution over model space is represented by a set of trees, which can be rather large and difficult to interpret. DensiTree is a tool that helps navigating these sets of trees.Results: The main idea behind DensiTree is to draw all trees in the set transparently. As a result, areas where a lot of the trees agree in topology and branch lengths show up as highly colored areas, while areas with little agreement show up as webs. This makes it possible to quickly get an impression of properties of the tree set such as well-supported clades, distribution of tMRCA and areas of topological uncertainty. Thus, DensiTree provides a quick method for qualitative analysis of tree sets.Availability: DensiTree is freely available from http://compevol.auckland.ac.nz/software/DensiTree/. The program is licensed under GPL and source code is available.Contact:remco@cs.auckland.ac.nz}
}

@article{lambertRobust2022,
  title = {{$R_\ast$}: A Robust {MCMC} Convergence Diagnostic with Uncertainty Using Decision Tree Classifiers},
  shorttitle = {{{R}}{${_\ast}$}},
  author = {Lambert, Ben and Vehtari, Aki},
  year = {2022},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {17},
  number = {2},
  pages = {353--379},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1252},
  abstract = {Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over the past three decades: mainly because of this, Bayesian inference is now a workhorse of applied scientists. Under general conditions, MCMC sampling converges asymptotically to the posterior distribution, but this provides no guarantees about its performance in finite time. The predominant method for monitoring convergence is to run multiple chains and monitor individual chains' characteristics and compare these to the population as a whole: if within-chain and between-chain summaries are comparable, then this is taken to indicate that the chains have converged to a common stationary distribution. Here, we introduce a new method for diagnosing convergence based on how well a machine learning classifier model can successfully discriminate the individual chains. We call this convergence measure R{${_\ast}$}. In contrast to the predominant R\textasciicircum, R{${_\ast}$} is a single statistic across all parameters that indicates lack of mixing, although individual variables' importance for this metric can also be determined. Additionally, R{${_\ast}$} is not based on any single characteristic of the sampling distribution; instead it uses all the information in the chain, including that given by the joint sampling distribution, which is currently largely overlooked by existing approaches. We recommend calculating R{${_\ast}$} using two different machine learning classifiers \textemdash{} gradient-boosted regression trees and random forests \textemdash{} which each work well in models of different dimensions. Because each of these methods outputs a classification probability, as a byproduct, we obtain uncertainty in R{${_\ast}$}. The method is straightforward to implement and could be a complementary additional check on MCMC convergence for applied analyses.},
  file = {/home/bolker/Zotero/storage/RX9L2CNB/Lambert and Vehtari - 2022 - R∗ A Robust MCMC Convergence Diagnostic with Unce.pdf}
}


@article{pagelDetecting1997,
	title = {Detecting correlated evolution on phylogenies: a general method for the comparative analysis of discrete characters},
	volume = {255},
	shorttitle = {Detecting correlated evolution on phylogenies},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.1994.0006},
	doi = {10.1098/rspb.1994.0006},
	abstract = {I present a new statistical method for analysing the relationship between two discrete characters that are measured across a group of hierarchically evolved species or populations. The method assesses whether a pattern of association across the group is evidence for correlated evolutionary change in the two characters. The method takes into account information on the lengths of the branches of phylogenetic trees, develops estimates of the rates of change of the discrete characters, and tests the hypothesis of correlated evolution without relying upon reconstructions of the ancestral character states. A likelihood ratio test statistic is used to discriminate between two models that are fitted to the data: one allowing only for independent evolution of the two characters, the other allowing for correlated evolution. Tests of specific directional hypotheses can also be made. The method is illustrated with an application to the Hominoidea.},
	number = {1342},
	urldate = {2023-01-15},
	journal = {Proceedings of the Royal Society of London. Series B: Biological Sciences},
	author = {Pagel, Mark},
	month = jan,
	year = {1997},
	note = {Publisher: Royal Society},
	pages = {37--45},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/38D2NIBW/Pagel - 1997 - Detecting correlated evolution on phylogenies a g.pdf:application/pdf},
}


@article{felsensteinMaximum1973,
	title = {Maximum {Likelihood} and {Minimum}-{Steps} {Methods} for {Estimating} {Evolutionary} {Trees} from {Data} on {Discrete} {Characters}},
	volume = {22},
	issn = {1063-5157},
	url = {https://doi.org/10.1093/sysbio/22.3.240},
	doi = {10.1093/sysbio/22.3.240},
	abstract = {The general maximum likelihood approach to the statistical estimation of phylogenies is outlined, for data in which there are a number of discrete states for each character. The details of the maximum likelihood method will depend on the details of the probabilistic model of evolution assumed. There are a very large number of possible models of evolution. For a few of the simpler models, the calculation of the likelihood of an evolutionary tree is outlined. For these models, the maximum likelihood tree will be the same as the “most parsimonious” (or minimum-steps) tree if the probability of change during the evolution of the group is assumed a priori to be very small. However, most sets of data require too many assumed state changes per character to be compatible with this assumption. Farris (1973) has argued that maximum likelihood and parsimony methods are identical under a much less restrictive set of assumptions. It is argued that the present methods are preferable to his, and a counterexample to his argument is presented. An algorithm which enables rapid calculation of the likelihood of a phylogeny is described.},
	number = {3},
	urldate = {2023-01-12},
	journal = {Systematic Biology},
	author = {Felsenstein, Joseph},
	month = sep,
	year = {1973},
	pages = {240--249},
}


@book{harmonPhylogenetic2019,
	edition = {1.4},
	title = {Phylogenetic {Comparative} {Methods}},
	url = {https://lukejharmon.github.io/pcm/pdf/phylogeneticComparativeMethods.pdf},
	language = {en},
	author = {Harmon, Luke J},
	month = mar,
	year = {2019},
	file = {Harmon - Phylogenetic Comparative Methods.pdf:/home/bolker/Documents/zotero_new/storage/6UXLFJMS/Harmon - Phylogenetic Comparative Methods.pdf:application/pdf},
}

@Article{TMB2016,
  title = {{TMB}: Automatic Differentiation and {L}aplace Approximation},
  author = {Kasper Kristensen and Anders Nielsen and Casper W. Berg and Hans Skaug and Bradley M. Bell},
  journal = {Journal of Statistical Software},
  year = {2016},
  volume = {70},
  number = {5},
  pages = {1--21},
  doi = {10.18637/jss.v070.i05},
}

@article{gelmanBayesian2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = 2020,
  month = nov,
  journal = {arXiv:2011.01808 [stat]},
  eprint = {2011.01808},
  primaryclass = {stat},
  urldate = {2020-11-04},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/bolker/Documents/zotero_new/storage/HYG8WGKB/Gelman et al. - 2020 - Bayesian Workflow.pdf;/home/bolker/Documents/zotero_new/storage/MK6YCMEY/2011.html}
}

@book{mcelreathStatistical2020,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = 2020,
  month = mar,
  publisher = {CRC Press},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {6H\_WDwAAQBAJ},
  isbn = {978-0-429-63914-2},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General}
}

@article{vehtariRankNormalization2019,
  title = {Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of {{MCMC}}},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = 2019,
  month = mar,
  journal = {arXiv:1903.08008 [stat]},
  eprint = {1903.08008},
  primaryclass = {stat},
  urldate = {2019-03-20},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic \$\textbackslash widehat\textbraceleft R\textbraceright\$ of Gelman and Rubin (1992) has serious flaws and we propose an alternative that fixes them. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give concrete recommendations for how these methods should be used in practice.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/bolker/Documents/zotero_new/storage/RSDGM5AU/Vehtari et al. - 2019 - Rank-normalization, folding, and localization An .pdf;/home/bolker/Documents/zotero_new/storage/ZF56W8WU/1903.html}
}

@article{vihola_robust_2012,
  title = {Robust Adaptive {{Metropolis}} Algorithm with Coerced Acceptance Rate},
  author = {Vihola, Matti},
  year = 2012,
  month = sep,
  journal = {Statistics and Computing},
  volume = {22},
  number = {5},
  pages = {997--1008},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-011-9269-5},
  urldate = {2021-09-29},
  abstract = {The adaptive Metropolis (AM) algorithm of Haario, Saksman and Tamminen (Bernoulli 7(2):223--242, 2001) uses the estimated covariance of the target distribution in the proposal distribution. This paper introduces a new robust adaptive Metropolis algorithm estimating the shape of the target distribution and simultaneously coercing the acceptance rate. The adaptation rule is computationally simple adding no extra cost compared with the AM algorithm. The adaptation strategy can be seen as a multidimensional extension of the previously proposed method adapting the scale of the proposal distribution in order to attain a given acceptance rate. The empirical results show promising behaviour of the new algorithm in an example with Student target distribution having no finite second moment, where the AM covariance estimate is unstable. In the examples with finite second moments, the performance of the new approach seems to be competitive with the AM algorithm combined with scale adaptation.},
  langid = {english},
  file = {/home/bolker/Documents/zotero_new/storage/2IMSFRNL/Vihola - 2012 - Robust adaptive Metropolis algorithm with coerced .pdf}
}

@book{burnhamModel2004,
  title = {Model {{Selection}} and {{Multimodel Inference}}},
  editor = {Burnham, Kenneth P. and Anderson, David R.},
  year = 2004,
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/b97636},
  urldate = {2025-03-20},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-0-387-95364-9},
  langid = {english},
  note = {http://link.springer.com/10.1007/b97636}
}

@article{makowskiIndices2019,
  title = {Indices of {{Effect Existence}} and {{Significance}} in the {{Bayesian Framework}}},
  author = {Makowski, Dominique and {Ben-Shachar}, Mattan S. and Chen, S. H. Annabel and L{\"u}decke, Daniel},
  year = 2019,
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  urldate = {2022-07-21},
  abstract = {Turmoil has engulfed psychological science. Causes and consequences of the reproducibility crisis are in dispute. With the hope of addressing some of its aspects, Bayesian methods are gaining increasing attention in psychological science. Some of their advantages, as opposed to the frequentist framework, are the ability to describe parameters in probabilistic terms and explicitly incorporate prior knowledge about them into the model. These issues are crucial in particular regarding the current debate about statistical significance. Bayesian methods are not necessarily the only remedy against incorrect interpretations or wrong conclusions, but there is an increasing agreement that they are one of the keys to avoid such fallacies. Nevertheless, its flexible nature is its power and weakness, for there is no agreement about what indices of ``significance'' should be computed or reported. This lack of a consensual index or guidelines, such as the frequentist p-value, further contributes to the unnecessary opacity that many non-familiar readers perceive in Bayesian statistics. Thus, this study describes and compares several Bayesian indices, provide intuitive visual representation of their ``behavior'' in relationship with common sources of variance such as sample size, magnitude of effects and also frequentist significance. The results contribute to the development of an intuitive understanding of the values that researchers report, allowing to draw sensible recommendations for Bayesian statistics description, critical for the standardization of scientific reporting.},
  note = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767},
  file = {/home/bolker/Documents/zotero_new/storage/2T3CMMW9/Makowski et al. - 2019 - Indices of Effect Existence and Significance in th.pdf}
}

@book{cummingIntroduction2017,
  title = {Introduction to the {{New Statistics}}: {{Estimation}}, {{Open Science}}, and {{Beyond}}},
  shorttitle = {Introduction to the {{New Statistics}}},
  author = {Cumming, Geoff and {Calin-Jageman}, Robert},
  year = 2017,
  publisher = {Routledge},
  address = {New York London},
  abstract = {This is the first introductory statistics text to use an estimation approach from the start to help readers understand effect sizes, confidence intervals (CIs), and meta-analysis (`the new statistics'). It is also the first text to explain the new and exciting Open Science practices, which encourage replication and enhance the trustworthiness of research. In addition, the book explains NHST fully so students can understand published research. Numerous real research examples are used throughout. The book uses today's most effective learning strategies and promotes critical thinking, comprehension, and retention, to deepen users' understanding of statistics and modern research methods. The free ESCI (Exploratory Software for Confidence Intervals) software makes concepts visually vivid, and provides calculation and graphing facilities. The book can be used with or without ESCI.Other highlights include:- Coverage of both estimation and NHST approaches, and how to easily translate between the two. - Some exercises use ESCI to analyze data and create graphs including CIs, for best understanding of estimation methods. -Videos of the authors describing key concepts and demonstrating use of ESCI provide an engaging learning tool for traditional or flipped classrooms.-In-chapter exercises and quizzes with related commentary allow students to learn by doing, and to monitor their progress.-End-of-chapter exercises and commentary, many using real data, give practice for using the new statistics to analyze data, as well as for applying research judgment in realistic contexts. -Don't fool yourself tips help students avoid common errors. -Red Flags highlight the meaning of "significance" and what p values actually mean. -Chapter outlines, defined key terms, sidebars of key points, and summarized take-home messages provide a study tool at exam time. -http://www.routledge.com/cw/cumming offers for students: ESCI downloads; data sets; key term flashcards; tips for using SPSS for analyzing data; and videos. For instructors it offers: tips for teaching the new statistics and Open Science; additional homework exercises; assessment items; answer keys for homework and assessment items; and downloadable text images; and PowerPoint lecture slides.Intended for introduction to statistics, data analysis, or quantitative methods courses in psychology, education, and other social and health sciences, researchers interested in understanding the new statistics will also appreciate this book. No familiarity with introductory statistics is assumed.},
  isbn = {978-1-138-82552-9},
  langid = {english}
}
